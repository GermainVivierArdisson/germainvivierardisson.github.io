
@article{vivier--ardisson_cf-opt_2024,
	title = {{CF}-{OPT}: {Counterfactual} {Explanations} for {Structured} {Prediction}},
	shorttitle = {{CF}-{OPT}},
	url = {http://arxiv.org/abs/2405.18293},
	abstract = {Optimization layers in deep neural networks have enjoyed a growing popularity in structured learning, improving the state of the art on a variety of applications. Yet, these pipelines lack interpretability since they are made of two opaque layers: a highly non-linear prediction model, such as a deep neural network, and an optimization layer, which is typically a complex black-box solver. Our goal is to improve the transparency of such methods by providing counterfactual explanations. We build upon variational autoencoders a principled way of obtaining counterfactuals: working in the latent space leads to a natural notion of plausibility of explanations. We finally introduce a variant of the classic loss for VAE training that improves their performance in our specific structured context. These provide the foundations of CF-OPT, a first-order optimization algorithm that can find counterfactual explanations for a broad class of structured learning architectures. Our numerical results show that both close and plausible explanations can be obtained for problems from the recent literature.},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Vivier-Ardisson, Germain and Forel, Alexandre and Parmentier, Axel and Vidal, Thibaut},
	month = may,
	year = {2024},
	note = {arXiv:2405.18293 [cs]},
	keywords = {Computer Science - Machine Learning},
	selected = {true},
	bibtex_show = {true},
	code = {https://github.com/GermainVivierArdisson/CF-OPT},
	preview = {pipeline_vae_last.png},
}
